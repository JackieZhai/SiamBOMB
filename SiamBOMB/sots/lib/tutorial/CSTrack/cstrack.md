# CSTrack tutorial

We assume the root path is $SOTS, e.g. `/home/chaoliang/SOTS`

## Set up environment

`$conda_path` denotes your anaconda path, e.g. `/home/chaoliang/anaconda3`
```
conda create -n CSTrack python=3.8
source activate CSTrack
cd SOTS/lib/tutorial/CSTrack/
pip install -r requirements.txt
```


## Testing
### Prepare data and models
1. Download the pretrained model[[Google Drive]](https://drive.google.com/file/d/1x0HUDD9t6mnHi3q2N3Uf4gtmBBwPvAd7/view?usp=sharing)[[Baidu NetDisk(oc33)]](https://pan.baidu.com/s/1vIi4aWw-uRT9fZBN2aksBg) to `$SOTS/weights`.
2. Download testing data e.g. MOT-16 and put them in `$SOTS/dataset`. The dataset can be downloaded from their [official webpage](https://motchallenge.net/).


### Testing
In root path `$SOTS/tracking`,

#### For MOT-16
```
python test_cstrack.py --nms_thres 0.6
                       --conf_thres 0.5
                       --weights $SOTS/weights/cstrack.pt
                       --data_dir $SOTS/dataset
                       --device 0
                       --test_mot16 True
                       --vis_state 0/1
```

#### For MOT-17
```
python test_cstrack.py --nms_thres 0.6
                       --conf_thres 0.5
                       --weights $SOTS/weights/cstrack.pt
                       --data_dir $SOTS/dataset
                       --device 0
                       --test_mot17 True
                       --vis_state 0/1
```

- Note: If you want to test the performance of the model. Please sign up at [MOT challange](https://motchallenge.net/) and test it.



:cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud::cloud:
## Training

### Prepare data and models

1. Download the pretrained model which pretrain in COCO dataset [[Google Drive]](https://drive.google.com/file/d/1qJHNlEXPVirDVmWL7hHeU4-P9amWHJHR/view?usp=sharing)[[Baidu NetDisk(ba1g)]](https://pan.baidu.com/s/1S04i6-yxQ3QHtfUDDtd1Kw) to `$SOTS/weights`.

2. We provide several relevant datasets for training and evaluating the CSTrack. 
Annotations are provided in a unified format and all the datasets have the following structure:

```
Caltech
   |——————images
   |        └——————00001.jpg
   |        |—————— ...
   |        └——————0000N.jpg
   └——————labels_with_ids
            └——————00001.txt
            |—————— ...
            └——————0000N.txt
```

Every image has a corresponding annotation text. Given an image path, 
the annotation text path can be generated by replacing the string `images` with `labels_with_ids` and replacing `.jpg` with `.txt`.

In the annotation text, each line is describing a bounding box and has the following format:
```
[class] [identity] [x_center] [y_center] [width] [height]
```
The field `[class]` should be `0`. Only single-class multi-object tracking is supported in this version. 

The field `[identity]` is an integer from `0` to `num_identities - 1`, or `-1` if this box has no identity annotation.

***Note** that the values of `[x_center] [y_center] [width] [height]` are normalized by the width/height of the image, so they are floating point numbers ranging from 0 to 1.


The datasets including Caltech, CityPersons, CUHK-SYSU, PRW, ETHZ and MOT-17 follow [JDE](https://github.com/Zhongdao/Towards-Realtime-MOT). 

#### Caltech Pedestrian
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1sYBXXvQaXZ8TuNwQxMcAgg)
[[1]](https://pan.baidu.com/s/1lVO7YBzagex1xlzqPksaPw) 
[[2]](https://pan.baidu.com/s/1PZXxxy_lrswaqTVg0GuHWg)
[[3]](https://pan.baidu.com/s/1M93NCo_E6naeYPpykmaNgA)
[[4]](https://pan.baidu.com/s/1ZXCdPNXfwbxQ4xCbVu5Dtw)
[[5]](https://pan.baidu.com/s/1kcZkh1tcEiBEJqnDtYuejg)
[[6]](https://pan.baidu.com/s/1sDjhtgdFrzR60KKxSjNb2A)
[[7]](https://pan.baidu.com/s/18Zvp_d33qj1pmutFDUbJyw)

Google Drive: [[annotations]](https://drive.google.com/file/d/1h8vxl_6tgi9QVYoer9XcY9YwNB32TE5k/view?usp=sharing) , 
please download all the images `.tar` files from [this page](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/datasets/USA/) and unzip the images under `Caltech/images`

You may need [this tool](https://github.com/mitmul/caltech-pedestrian-dataset-converter) to convert the original data format to jpeg images.
Original dataset webpage: [CaltechPedestrians](http://www.vision.caltech.edu/Image_Datasets/CaltechPedestrians/)
#### CityPersons
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1g24doGOdkKqmbgbJf03vsw)
[[1]](https://pan.baidu.com/s/1mqDF9M5MdD3MGxSfe0ENsA) 
[[2]](https://pan.baidu.com/s/1Qrbh9lQUaEORCIlfI25wdA)
[[3]](https://pan.baidu.com/s/1lw7shaffBgARDuk8mkkHhw)

Google Drive:
[[0]](https://drive.google.com/file/d/1DgLHqEkQUOj63mCrS_0UGFEM9BG8sIZs/view?usp=sharing)
[[1]](https://drive.google.com/file/d/1BH9Xz59UImIGUdYwUR-cnP1g7Ton_LcZ/view?usp=sharing) 
[[2]](https://drive.google.com/file/d/1q_OltirP68YFvRWgYkBHLEFSUayjkKYE/view?usp=sharing)
[[3]](https://drive.google.com/file/d/1VSL0SFoQxPXnIdBamOZJzHrHJ1N2gsTW/view?usp=sharing)

Original dataset webpage: [Citypersons pedestrian detection dataset](https://bitbucket.org/shanshanzhang/citypersons)

#### CUHK-SYSU
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1YFrlyB1WjcQmFW3Vt_sEaQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1D7VL43kIV9uJrdSCYl53j89RE2K-IoQA/view?usp=sharing)

Original dataset webpage: [CUHK-SYSU Person Search Dataset](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)

#### PRW
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1iqOVKO57dL53OI1KOmWeGQ)

Google Drive:
[[0]](https://drive.google.com/file/d/116_mIdjgB-WJXGe8RYJDWxlFnc_4sqS8/view?usp=sharing)

Original dataset webpage: [Person Search in the Wild datset](http://www.liangzheng.com.cn/Project/project_prw.html)

#### ETHZ (overlapping videos with MOT-16 removed):
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/14EauGb2nLrcB3GRSlQ4K9Q)

Google Drive:
[[0]](https://drive.google.com/file/d/19QyGOCqn8K_rc9TXJ8UwLSxCx17e0GoY/view?usp=sharing)

Original dataset webpage: [ETHZ pedestrian datset](https://data.vision.ee.ethz.ch/cvl/aess/dataset/)

#### MOT-17
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/1lHa6UagcosRBz-_Y308GvQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1ET-6w12yHNo8DKevOVgK1dBlYs739e_3/view?usp=sharing)

Original dataset webpage: [MOT-17](https://motchallenge.net/data/MOT17/)

#### MOT-16 (for evaluation )
Baidu NetDisk: 
[[0]](https://pan.baidu.com/s/10pUuB32Hro-h-KUZv8duiw)

Google Drive:
[[0]](https://drive.google.com/file/d/1254q3ruzBzgn4LUejDVsCtT05SIEieQg/view?usp=sharing)

Original dataset webpage: [MOT-16](https://motchallenge.net/data/MOT16/)


#### CrowdHuman
The CrowdHuman dataset can be downloaded from their [official webpage](https://www.crowdhuman.org). The annotation text can be downloaded from the following Baidu NetDisk and Google Drive we provide. 

Baidu NetDisk: 
[[l77e]](https://pan.baidu.com/s/1-wlHeQwizqTN7Ce1tkrXTQ)

Google Drive:
[[0]](https://drive.google.com/file/d/1Q0obzf3WFjFq6bHFjkiHCgRYjU-tUw6d/view?usp=sharing)

Original dataset webpage: [CrowdHuman](https://www.crowdhuman.org)

The CrowdHuman dataset has the following structure:
```
crowdhuman
   |——————images
            |——————train
            |        └——————00001.jpg
            |        |—————— ...
            |        └——————0000N.jpg
            |——————val
            |        └——————00001.jpg
            |        |—————— ...
            |        └——————0000N.jpg
   └——————labels_with_ids
            |——————train
            |        └——————00001.txt
            |        |—————— ...
            |        └——————0000N.txt
            |——————val
            |        └——————00001.txt
            |        |—————— ...
            |        └——————0000N.txt
```

### Training
1. Modify scripts
Set the dataset path in line2 of `$SOTS/lib/dataset/mot/cfg/*.json`.

2. run
```
python train_cstrack.py --batch_size 8 --device 0 --weights $SOTS/weights/yolov5l.pt --epochs 30 --data ../lib/dataset/mot/cfg/data_ch.json #train on all datasets
                                                                                                ../lib/dataset/mot/cfg/data.json #train on datasets like JDE
                                                                                                ../lib/dataset/mot/cfg/mot17.json #train on MOT17 training set
                                                                                                ../lib/dataset/mot/cfg/mot17_hf.json #train on the half of MOT17 training set
```

## References
```
[1] Z. Wang, L. Zheng, et al. Towards real-time multi object tracking. ECCV2020.
[2] Yolov5. https://github.com/ultralytics/yolov5.
```
